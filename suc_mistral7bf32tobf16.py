# -*- coding: utf-8 -*-
"""suc_mistral7bF32toBF16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14kx-45MConyoiIuee5vzZvLsqncXIaCH
"""



!pip install llama-cpp-python

# !pip install llama-cpp-python

from llama_cpp import Llama

llm = Llama.from_pretrained(
	repo_id="rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF",
	filename="mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf",
)

llm.create_chat_completion(
	messages = "No input example has been defined for this model task."
)

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf

!wget https://huggingface.co/rakmik/Mistral-7B-Instruct-v0.3-Q8_0-GGUF/resolve/main/mistral-7b-instruct-v0.3-q8_0-00002-of-00002.gguf

from llama_cpp import Llama

llm = Llama(model_path="/content/mistral-7b-instruct-v0.3-q8_0-00001-of-00002.gguf")
response = llm.create_chat_completion(
  messages=[
    {
        "role": "user",
        "content": "how big is the sky"
    }
])
print(response)



from llama_cpp import Llama

llm = Llama(
      model_path="./models/7B/llama-model.gguf",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)

!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-f32.gguf

!wget https://github.com/ggml-org/llama.cpp/releases/download/b6123/llama-b6123-bin-ubuntu-x64.zip

!unzip llama-b6123-bin-ubuntu-x64.zip

!./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

!build/bin/llama-quantize -h

!build/bin/llama-quantize /content/Mistral-7B-Instruct-v0.3-f32.gguf /content/Mistral-7B-Instruct-v0.3-bf16.gguf BF16

!/content/build/bin/llama-gguf-split -h

!build/bin/llama-cli -h

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-f32.gguf

xxxxxxx

/content/Mistral-7B-Instruct-v0.3-bf16.gguf

# 1. تثبيت المكتبة
#!pip install huggingface_hub

# 2. تسجيل الدخول
from huggingface_hub import notebook_login
notebook_login()

# 3. رفع الملف
from huggingface_hub import HfApi

# تأكد من أن هذا المسار صحيح وموجود في بيئة Colab الخاصة بك
file_path = "/content/Mistral-7B-Instruct-v0.3-bf16.gguf"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# تأكد من إنشاء هذا المستودع على حسابك في Hugging Face أولاً
# يمكنك إنشاؤه من هنا: https://huggingface.co/new

try:
    api = HfApi()
    api.upload_file(
        path_or_fileobj=file_path,
        path_in_repo="Mistral-7B-Instruct-v0.3-GGUF_F16", # اسم الملف في المستودع
        repo_id=repo_id,
        repo_type="model"
    )
    print(f"تم رفع الملف بنجاح! يمكنك مشاهدته هنا: https://huggingface.co/{repo_id}/tree/main")

except Exception as e:
    print(f"حدث خطأ أثناء الرفع: {e}")

!build/bin/llama-cli -m /content/Mistral-7B-Instruct-v0.3-bf16.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

!/content/build/bin/llama-gguf-split -h

!/content/build/bin/llama-gguf-split --split-max-size 5G  /content/Mistral-7B-Instruct-v0.3-bf16.gguf

!/content/build/bin/llama-gguf-split --split-max-size 5G /content/Mistral-7B-Instruct-v0.3-bf16.gguf /content/Mistral-7B-Instruct-v0.3-bf16-split.gguf

from huggingface_hub import HfApi

# تهيئة الواجهة البرمجية
api = HfApi()

# تحديد المجلد المحلي واسم المستودع
folder_path = "/content/3parts"
repo_id = "rakmik/Mistral-7B-Instruct-v0.3-GGUF_F16"

# رفع المجلد
# سيتم رفع محتويات المجلد مباشرة إلى المستودع
api.upload_folder(
    folder_path=folder_path,
    repo_id=repo_id,
    repo_type="model"
)

print(f"تم رفع المجلد بنجاح إلى المستودع: https://huggingface.co/{repo_id}/tree/main")

!build/bin/llama-cli -m /content/3parts/Mistral-7B-Instruct-v0.3-bf16-split.gguf-00001-of-00003.gguf -p "I believe the meaning of life is" -n 128 -no-cnv